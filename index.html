<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">


    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <title>A Practical Guide to Optimization under Uncertainty</title>
    <meta http-equiv="Content-Style-Type" content="text/css" />
    <meta name="generator" content="pandoc" />
    <meta name="author" content="M. Mohsen Moarefdoost" />
    <link href='https://fonts.googleapis.com/css?family=Lato' rel='stylesheet' type='text/css'>
    <style type="text/css">code{white-space: pre;}</style>
    <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
    </script>
    <script src="/theme/js/jquery-2.1.3.min.js"></script>




    <script type="text/javascript">
      WebFontConfig = {
        google: { families: [ 'Lato::latin' ] }
      };
      (function() {
        var wf = document.createElement('script');
        wf.src = 'https://ajax.googleapis.com/ajax/libs/webfont/1/webfont.js';
        wf.type = 'text/javascript';
        wf.async = 'true';
        var s = document.getElementsByTagName('script')[0];
        s.parentNode.insertBefore(wf, s);
      })();
    </script>

    <style type="text/css">

        @import url(https://fonts.googleapis.com/css?family=Lato);

        body {

        font-family: 'Lato', sans-serif !important;
        }

        p.mrg{
        padding-top: 25px;
        padding-right: 110px;
        padding-bottom: 25px;
        padding-left: 110px;
        }

        p.mrgEnd{
        padding-top: 10px;
        padding-right: 50px;
        padding-bottom: 30px;
        padding-left: 50px;
        }

        p.borderMrg{
        border: 1px solid red;
        background-color: yellow;
        padding-top: 50px;
        padding-right: 30px;
        padding-bottom: 50px;
        padding-left: 30px;
        }

        pre code {
          background-color: #eee;
          border: 1px solid #999;
          display: block;
        margin-right: 110px;
        margin-left: 110px;
        }

        h1 {
            display: block;
            margin-top: 50px;
            margin-bottom: 10px;
            font-size:40px;
            color: 0F0401
        }

        h2{
        text-indent: 75px;
        font-size:36px;
        color: 371309
        }
        h3{
        text-indent: 85px;
        font-size:28px;
        color: 371309
        }

        h4{
        text-indent: 95px;
        font-size:24px;
        color: 371309
        }

        h5{
        text-indent: 105px;
        font-size:20px;
        color: 371309
        }


        ol{
        padding-right: 110px;
        padding-left: 110px;
        LINE-HEIGHT:25px
        }

        ul{
        padding-top: 10px;
        padding-right: 110px;
        padding-left: 110px;
        LINE-HEIGHT:25px;
        }

        p{
        text-align: justify;LINE-HEIGHT:25px

        }
    </style>
</head>
<font color="red"; size="20pt">Note: This document is a work in progress and not complete</font>
    <h1 align="center";>A Practical Guide to Optimization under Uncertainty</h1>
    <p class="mrg">
    Optimization under Uncertainty has lots of used cases in many real world
    and practical problems in the area of supply chain, transportation,
    retail and finance, etc. However, current available resources provide limited
    help to industry practitioners.<br />
    If you look at some online resources on Two-stage Stochastic Programming,
    Dynamic Programming and Robust Optimization, you see they get too technical
    too soon which puts a burden on their applicability.
    In this guide, we are providing a simple tutorial on how to
    solve optimization problems (mainly linear and mixed integer linear programming)
    when we face some uncertainty. We give a simple practical example and show a practical way of
    formulating that problem in the presence of uncertainty, provide
    useful algorithms and present sample codes in Python for reference.
    </p>
    <h2> Prerequisites </h2>
    <p class="mrg">
    We assume a reader of this tutorial is familiar with basics Linear and Mixed Integer Programming,
    and knows what <em>Decision Variables</em>, <em>Constraints</em>, <em>Objective Function</em>,
    and <em>Non-negativity restriction</em> are.
    </p>

    <h2>Modeling Uncertainty</h2>
    <p class="mrg">
    Before explaining possible approaches one may take to solve an optimization problem under uncertainty,
    let us refresh our memory on deterministic LP/MIP problems.
    A typical LP/MIP problem has a mathematical form like this:</p>

    <div style="left;" class="math display">
    \[
    \left. \begin{array}{lll}
    \text{min}& c_1x_1+c_2x_2+\cdots+c_nx_n&\\
    \text{s.t.}\\
    &a_{11}x_{11}+a_{12}x_{12}+\cdots+x_{1n}x_{1n}&\leq b_1\\
    &\vdots&\vdots\\
    &a_{m1}x_{m1}+a_{m2}x_{m2}+\cdots+x_{mn}x_{mn}&\leq b_m\\
    &x_1, x_2, \cdots, x_n \geq 0&\\
    &\text{Some}\ x_i\text{s are Integer}
    \end{array}
    \right.
    \]</div>


    <p class="mrg">
        Here, <span class="math inline">$c_1, c_2, \cdots, c_n$</span>
        are objective coefficient parameters,
        <span class="math inline">$a_{11}, a_{12}, \cdots, a_{nm}$</span> are constraints coefficient parameters,
        and <span class="math inline">$b_{1}, b_{2}, \cdots, b_{m}$</span>
        are right hand side parameters. Either of these parameters
        or all of them can be uncertain or random, and you may have their probability distribution, or not.
        Depending on which set is random, one might adopt different approach to model and solve the underlying problem.

        In general, there are four approaches to optimization under uncertainty:</p>
        <ol>
            <li><b>Robust Optimization Models</b></li>
            In these types of models, one consider the worst possible outcome and optimize decisions based on that.
            As mentioned <a href="https://dspace.mit.edu/openaccess-disseminate/1721.1/66198">here</a>,
            in robust optimization the uncertainty is not stochastic, but rather deterministic and set-based.
            We are not covering robust optimization in this tutorial.
            There are plenty of good resources online that interested readers may consult with.
            <br>
            <br>
            <li><b>Deterministic Equivalent Models</b></li>
            In these types of models, we replace the problem with uncertain parameters
            with a deterministic equivalent formulation. These models are generally easy to understand
            and some of them are practical. There are three common methods to obtain
            the deterministic equivalence model:
            <ul>
                <li><b>Estimate method</b></li>
                This is the simplest approach to deal with uncertain parameters in a mathematical optimization problem
                where we set a deterministic value for each uncertain parameter such as their mean or
                their quantile estimation, depending on business requirements. For example, assume that
                a minimization objective
                function like <span class="math inline">$$\textbf{min}\ c_1x_1+\cdots+c_nx_n$$</span>
                has uncertain coefficients, and <span class="math inline">$c_1, c_2, \cdots, c_n$</span> are normal iid
                random variables
                with mean  <span class="math inline">$\mu_i, i=1,2,\cdots,n$</span>,
                and standard deviation <span class="math inline">$\sigma_{i}, i=1,2,\cdots,n$</span>.
                Here, one can ignore variability and replace uncertain objective function
                with <span class="math inline">$$\textbf{min}\ \mu_1x_1+\cdots+\mu_nx_n.$$</span>
                Or, one might be interested in minimizing when uncertain parameters are at 95%-quantile
                (maximizing 5%-quantile when objective function is maximization)
                which leads to <span class="math inline">$$\textbf{min}\ (\mu_1+\mathcal{z}_{0.95}\sigma_1)x_1+
                \cdots+(\mu_n+\mathcal{z}_{0.95}\sigma_n)x_n.$$</span>

                Note that this minimizing quantile value problem becomes nonlinear when <span class="math inline">$c_1, c_2, \cdots, c_n$</span>
                are NOT iid random variables. Assume that <span class="math inline">$c_1, c_2, \cdots, c_n$</span>
                have the multivariate normal distribution with mean vector <span class="math inline">$\mu=[\mu_i], i=1,2,\cdots,n$</span>,
                and covariance matrix <span class="math inline">$\Sigma=[\sigma_{ij}], i,j=1,2,\cdots,n$</span>. Therefore,
                the above objective function will be normally distributed with mean
                <span class="math inline">$\mu_1x_1+\cdots+\mu_nx_n$</span> and
                variance <span class="math inline">$\sum_{i=1}^{n}{\sigma_{ii}x^2_i}+2\sum_{i=1}^{n}\sum_{j=i+1}^{n}{\sigma_{ij}x_ix_j}$</span>.
                Then, objective function for minimizing 95%-quantile is
                <span class="math inline">$$\textbf{min}\ \mu_1x_1+\cdots+\mu_nx_n +
                \mathcal{z}_{0.95} \sqrt{\sum_{i=1}^{n}{\sigma_{ii}x^2_i}+2\sum_{i=1}^{n}
                    \sum_{j=i+1}^{n}{\sigma_{ij}x_ix_j}},$$</span>
                which is not linear anymore.
                This <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.534.3911&rep=rep1&type=pdf">reference</a>
                provides more technical details of how one can solve this optimization problem.
                <br><br>
                The above reformulation is for cases where objective function is uncertain.
                However, there are many problems in practice that we have uncertainty in constraints.
                Here, we explain how to deal with inequality constraints. For equality constraint,
                this method does not produce meaningful deterministic formulation.
                <em>Expected violation penalty method</em> which we explain later is a better
                choice when we have uncertainty in equality constraints. Now, assume an inequality constraint like
                <span class="math inline">$$a_1x_1+\cdots+a_nx_n \leq b$$</span>
                has uncertain coefficients, and <span class="math inline">$a_1, a_2, \cdots, a_n$</span> are normal iid
                random variables
                with mean  <span class="math inline">$\mu_i, i=1,2,\cdots,n$</span>,
                and standard deviation <span class="math inline">$\sigma_{i}, i=1,2,\cdots,n$</span>.
                Again, one may ignore variability and replace uncertain parameters with their mean and get
                with <span class="math inline">$$\mu_1x_1+\cdots+\mu_nx_n \leq b.$$</span>
                Or, one might be interested in cases where this constraint is satisfied
                when uncertain parameters are at 95%-quantile
                (5%-quantile for <span class="math inline">$\geq$</span> inequality sign.)
                which leads to <span class="math inline">$$ (\mu_1+\mathcal{z}_{0.95}\sigma_1)x_1+
                \cdots+(\mu_n+\mathcal{z}_{0.95}\sigma_n)x_n\leq b.$$</span>
                Same as what we observe for uncertain objective function where <span class="math inline">$a_1, a_2, \cdots, a_n$</span>
                are NOT iid random variables, the formulation is different. In the next section,
                <em>Chance constraint optimization</em>, we explain how to deal with such cases.
                <br><br>
                <li><b>Chance constraint optimization</b></li>
                In this approach, we estimate or guess the highly possible outcome of random parameters,
                or optimize under certain confidence level. <br><br>
                <li><b>Expected violation penalty method</b></li>
                This method is well suited for problems where uncertainty is present in constraints.
                In the case of uncertain constraints, we are interested in containing or minimizing their violations
                rather than making sure if they are satisfied or not. In other word, uncertain constraints are treated as soft constraints.
                <br>
                For an inequality constraint like
                <span class="math inline">$a_1x_1+\cdots+a_nx_n \leq b$</span>
                violation is defined as <span class="math inline">$\delta = \text{max}\{0, a_1x_1+\cdots+a_nx_n -b\},$</span>
                <br>
                and for an equality constraint like <span class="math inline">$a_1x_1+\cdots+a_nx_n = b$</span>
                violation is defined as
                <span class="math inline">$\delta = \text{max}\{0, a_1x_1+\cdots+a_nx_n -b\}
                    +\text{max}\{0, b - a_1x_1+\cdots+a_nx_n\}$</span>.<br>
                In the presence of uncertain parameters, either on <span class="math inline">$a_i$s</span>
                or <span class="math inline">$b$</span>, we may be interested
                in minimizing the expected value of violation or making sure the violation's expected value is bounded, i.e.,
                    <div style="left;" class="math display">
                        \[
                        \left. \begin{array}{l}
                        \text{min} \quad E[\delta]\\
                        \text{or}\\
                        E[\delta] \leq \epsilon
                        \end{array}
                        \right.
                        \]
                    </div>
                Here we need to find analytical form of <span class="math inline">$E[\delta]$</span>.
                But it is usually a difficult task and in some cases impossible,
                also there is no guarantee of linearity even if you can find the right analytical form.
                Lets explain it with a simple example where we are dealing with an
                inequality constraint with uncertain right hand side, i.e.,
                we have <span class="math inline">$a_1x_1+\cdots+a_nx_n \leq b$</span>
                where <span class="math inline">$b$</span> is uncertain. For simplicity assume
                <span class="math inline">$b$</span> is normally distributed with
                mean <span class="math inline">$\mu$</span> and variance <span class="math inline">$\sigma^2$</span>.
                Therefore,
                <div style="left;" class="math display">
                        \[
                        \left. \begin{array}{ll}
                        E[\delta]&=E[\text{max}\{0, a_1x_1+\cdots+a_nx_n -b\}]\\
                        &= \int_{-\infty}^{a_1x_1+\cdots+a_nx_n}{(a_1x_1+\cdots+a_nx_n-b)f(b)db}\\
                        &= \sigma [\mathcal L(z)+z]\\
                        \end{array}
                        \right.
                        \]
                </div>
                where
                <span class="math inline">$z = \frac{a_1x_1+\cdots+a_nx_n-\mu}{\sigma}$</span> and
                <span class="math inline">$\mathcal L(z)$</span> is the <em>Standard Normal Loss Function.</em>
                It is clear that even in the simplest case of single variable normal distribution,
                <span class="math inline">$E[\delta]$</span> is not linear. However, we can draw large enough
                sample for <span class="math inline">$b$</span> and approximate <span class="math inline">$E[\delta]$</span>.
                Here is how:<br>
                Lets assume that <span class="math inline">$b=b_k$</span> with
                <span class="math inline">$Pr(b=b_k)=\pi_k$</span> where <span class="math inline">$k=1,2,\cdots, K.$</span>
                Then, we have:
                <div style="left;" class="math display">
                        \[
                        \left. \begin{array}{ll}
                        E[\text{max}\{0, a_1x_1+\cdots+a_nx_n -b\}]\approx\sum_{k=1}^{K}{\pi_kv_k}&\\
                        \text{where}&\\
                        v_k\geq 0&\forall k\\
                        v_k\geq a_1x_1+\cdots+a_nx_n -b_k&\forall k\\
                        \end{array}
                        \right.
                        \]
                </div>
                This approximation increases the size of problem but preserves the linearity.
                <br><br>

            </ul>

            <li><b>Recourse Models</b></li>
            In this approach generally you can take corrective actions when you observe
            the true value of an uncertain parameter.
            <ul>
                <li>Two-Stage Models</li>
                <br><br>
                <li>Multi-Stage Models</li>
                    <p class ="mrg">Nonanticipativity:</br>
        A nonanticipativity constraint is a constraint that limits the
        choice of the decisions to the information that has been revealed so far</p>
            </ul>
            <li><b>Dynamic Programming</b></li>
        </ol>



    <h3>A Simple Example</h3>
    <p class="mrg">
    To show different ways of dealing with uncertainty, we explain each of the above mentioned
        methods in details using the following simple example.</br>
    Consider a production plant which produces <span class="math inline">$N$</span> products. This plant can either produce,
    or source this product to satisfy customers' monthly demand
    <span class="math inline">$D_{nt}, \ t=1,2,\cdots,12, n=1,2,\cdots,N$</span> for a year.
    The unit production and sourcing costs are <span class="math inline">$c^p_{n}, \ n=1,2,\cdots,N$</span> and
    <span class="math inline">$c^s_{n}, \  n=1,2,\cdots,N$</span>, respectively.
    In case of over production
    in month <span class="math inline">$t$</span> , it can hold inventory at cost
    <span class="math inline">$c^h_{n}, \  n=1,2,\cdots,N$</span>.
    This plant has an efficiency rate of <span class="math inline">$r_{n}\leq 1, \ n=1,2,\cdots,N$</span>.
    Total production capacity in each month depends on the availability of raw material.
    <span class="math inline">$C_{t}, \ t=1,2,\cdots,12$</span> is the total monthly capacity in units.
    We are looking for the best set of sourcing and production decisions that minimizes total sourcing, holding and production costs.
    Let <span class="math inline">$X_{t}, \ t=1,2,\cdots,12$</span> and
    <span class="math inline">$S_{t}, \ t=1,2,\cdots,12$</span>
     be decision variables representing
    monthly production and sourcing amount, respectivly. In addition,
    we define <span class="math inline">$I_{nt}, \ t=1,2,\cdots,12$</span> as inventory level in month
    <span class="math inline">$t$</span> for product <span class="math inline">$n$</span>
    where <span class="math inline">$I_{n0}=\bar{I}_{n}$</span> is the starting inventory
    for product <span class="math inline">$n$</span>.
    Here is the mathematical formulation:
    </p>
    <div style="left;" class="math display">
    \[
    \left. \begin{array}{lll}
    \text{min}& \sum_{n=1}^{N}\sum_{t=1}^{12}{c^p_{n}X_{nt}+c^s_{n}S_{nt}+c^h_{n}I_{nt}}&\\
    \text{s.t.}\\
    &\sum_{n=1}^{N}{r_{n}x_{nt}}\leq C_{t}&\forall t\\
    &I_{nt-1}+S_{nt} + X_{nt} - I_{nt}= D_{nt}&\forall t\quad \forall n\\
    &S_{nt}, X_{nt}, I_{nt} \geq 0&\forall t\quad \forall n\\
    &I_{n0}=\bar{I}_{n}&\forall n
    \end{array}
    \right.
    \]</div>

    <h2> Optimization Algorithms</h2>
    <p class ="mrg">Here we provide some helpful algorithms that come handy when dealing
    with optimization under uncertainty.
    <h3>Bender’s Decomposition </h3>
    <h3>Lagrangian Relaxation </h3>

    <h2>Authors </h2>

    <p class ="mrg">* **Mohsen Moarefdoost, Ph.D.** </p>


    <pre>
      <code>

          list = pd.Datafram({})
          <font color="blue">def</font> my_fun(x, y):
            x = x+ y
            <font color="blue">return</font> x
      </code>
    </pre>

</html>